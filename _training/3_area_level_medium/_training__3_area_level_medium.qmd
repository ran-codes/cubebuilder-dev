---
title: '_training__3_area_level_medium'
author: 
  - name: Renovator Name
date: last-modified
format: 
  html:
    toc: true
    toc-expand: true
    self-contained: true
    code-fold: false
    df-print: kable
    code-tools: true
    code-block-border-left: '#39b69d'
    code-block-bg: '#ebf0ef'
comments:
  hypothesis: 
    theme: clean
editor: source
execute:  
  warning: false
  message: false
  eval: true
editor_options: 
  chunk_output_type: console
---

# 1. Data Standardization

## 1.1 Setup

We first fill out the configurations here. 

```{r}
#| code-summary: 'Setup'
#| code-fold: true

{ ## 'Global Setup (For any notebook)'
  
  # Local Training Folder Relative path to repository root (this training repository is self contained - but you can configure to any shared folder or cloud storage and this will work)
  record_path = here::here('_training/3_area_level_medium')
  setwd(record_path)

  #### Configure relative path
  path_global_setup_function = here::here('R/setup/global_setup.R')
  
  #### Generic global setup code (Do not change!)
  source(path_global_setup_function)
  setup = global_setup(path_global_setup_function)
  invisible(list2env(setup$sourced_objects, envir = .GlobalEnv))
  global_context = setup$global_context
}

{ ## Renovation notebook specific setup
  
  #### Configurations for each renovation (Change!)
  local_config = lst(
    ## Composite keys (Required)
    dataset_id_tmp = "3_area_level_medium", # Dataset ID name. Example: 'LE_L1'
    dataset_version_tmp = 'v1', # Dataset version. Example: 'v1.0'
    observation_type_tmp = 'area-level',
    schema_version_tmp = 'v2', # Schema version. Example: 'v2'
    ## Processing info (Required)
    raw_dataset_dir = file.path(global_context$path_shared_home,'1_unstandardized/3_dummy_dataset_medium_area'), # Unencrypted server data location. Example: 'Data/Mortality Data/Life Expectancy/'
    vec__var_names_to_remove = c(''), # Optional, leave empty if not needed. Variables to exclude from data operationalization.
    file_list_tmp = c("dummy_L1AD_LE_MEDIAN.csv") %>% # List all data files to be operationalized. Example: 'LE_L1_20200824.csv'
      list(),
    ## Renovation metadata (Required)
    dataset_renovation_maintainer = 'User', # Maintainer name
    dataset_renovation_context_contributor = '', # Context Contributer name
    dataset_renovation_flags = c("") # ?????
  )
  
  ## Get list of files to freeze (scratch code)
  file.path(local_config$raw_dataset_dir) %>%
    list.files() %>% 
    discard(~str_detect(.x, 'Codebook_')) 
  
  #### Generic renovation notebook setup code (Do not change!)
  context = get_renovation_notebook_context(local_config, global_context) 
  
}

```

If all is well then get_renovation_notebook_context() will set things up the renovation infrastructure for you:

- Folder Setup: setups a server fodler for this renovation
- Dependencies: imports all libraries and functions you will need
- Raw data: freezes raw data into long term stable storage and compiles raw data into `context$raw_data`
- Logs: It has recorded all of the actions taken in JSON logs for version control (check you diff)

Now we are ready to renovate. Let's first examine the raw data.

```{r}
#| code-summary: 'Brain - Raw data preview'
#| code-fold: true

## Preview raw data
dfa = context$raw_data %>% 
  collect() %>% 
  as_tibble()

glimpse(dfa)
```

Looks like a SALURBAL dataset. Let's start tidying.

## 1.2 Declare Variables

Here we specify what variables are in the data set. Removing composite keys and redundant variables. For this dataset we only have one variable: LE_MEDIAN.

The final data structure is a table which is one row per original variable name then a standardized variable name.

Tip: AI is actually a great place to to this. If you want you can try copying the glimpse() results from above and ask AI to adopt the template code below. For example the code below was generated completely from the Renovations trained AI. But what ever works for you as long as the result is the same.

```{r}
#| code-summary: 'Brain - Variables processing'
## Declare the variables
df_var_name = tibble(var_name_raw = context$raw_data %>% 
                       select(-c(file_data, schema_version, dataset_version, dataset_id,
                                 observation_type, dataset_instance, geo,
                                 # Remove administrative columns
                                 ISO2, SALID1, YEAR, MONTH, DAY,
                                 # Remove name columns
                                 L1_NAME)) %>% 
                       names()) %>% 
  ## Variable sanitation
  rowwise() %>% 
  mutate(var_name = sanitize_codebook_var(var_name_raw)) %>% 
  ungroup() %>%
  ## Re op. composite keys for variable table
  mutate(
    dataset_id = context$dataset_id_tmp,
    dataset_instance = context$dataset_instance_tmp,
    dataset_version = context$dataset_version_tmp,
    schema_version = context$schema_version_tmp
  ) %>%
  ## Standard arrange and select
  arrange(var_name, var_name_raw, dataset_id) %>% 
  select(
    all_of(context$vec__admin_variable_definition_table_columns)
  ) %>%
  distinct() 



## Preview
salurbal_renovation_reactable(df_var_name)
```

Here we have a validation funciton that checks the structure and will give you tips if its not right.

```{r}
#| code-summary: 'Automated - QC'

# Check for UTF8 encoding, valid dataset variables, valid primary key, unique var_name, etc.
process_variable_table(df_var_name, context)
```

Looks good. This also generate JSON logs so we can track changes in our git diffs; this is a good place to commit your work. 


## 1.3 Declare Strata

Just looking at the data we know its not stratified as its area level air pollution data. Let's use the provided funciton to generate an empty strata table. 

```{r}
#| code-summary: 'Brain - Strata processing'
df_strata = generate_empty_df_strata(df_var_name, context)
salurbal_renovation_reactable(df_strata)
```

Let's run validations on the strata table.

```{r}
#| code-summary: 'Automated - QC'
process_strata_table(df_strata, context)
```


## 1.4 Data Cube

### Processing

Here we tidy the raw data into a data cube. This is one of the main steps of the renovations. Template code can be modified to tidy. Tip: if you are using AI provide: 1) code template below 2) var_name.json 3) strata.json 4) glimpse(df_data_raw) results; usually you get very good results. Tip2: use glimpse() throughout your pipeline!

```{r}
if (!file.exists(context$path_cache_int_data)){
  
  
  df_data_raw = context$raw_data %>% collect() %>% as_tibble()
  glimpse(df_data_raw)
  
  df_data_int = df_data_raw %>%
    ## Operationalize observation_id
    mutate(observation_id = SALID1) %>%
    select(-SALID1) %>% 
    ## Remove non-composite-key and non-variable columns
    select(-L1_NAME) %>% 
    ## Standardize existing composite keys
    rename(
      iso2 = ISO2,
      year = YEAR,
      month = MONTH, 
      day = DAY) %>% 
    ## Pivot longer to reshape life expectancy measurement
    pivot_longer(
      cols = c(LE_MEDIAN),
      names_to = 'var_name_raw',
      values_to = 'value'
    ) %>%  
    ## Filter out missing values and ensure we only process relevant variables
    filter(!is.na(value),
           var_name_raw %in% df_var_name$var_name_raw) %>% 
    mutate_all(~as.character(.x)) %>% 
    ## merges with variable and strata definitions
    left_join(df_var_name) %>%
    left_join(df_strata) %>%
    ## Standardize Selections
    select(
      all_of(context$vec__admin_composite_keys_all),
      all_of(context$vec__admin_data_columns_never_empty),
      any_of(context$vec__admin_data_columns_all)
    )
  

  ## Cache preliminary results to enable out-of-memory work
  df_data_int %>% write_parquet(context$path_cache_int_data)
  rm(df_data_int)
  gc()
  
} 

```

### Validation

No we run through our validation funciton that checks for consistency and the correct structure.

```{r}
if (file.exists(context$path_cache_staged_data)) {
  final_data_cube = open_dataset(context$path_cache_staged_data)
} else {
  
  ## Validate
  df_data_int = context$path_cache_int_data %>% 
    open_dataset() %>% 
    collect()
  validated_int_data_cube = df_data_int %>% 
    validate_final_admin_data_cube(., context)
  gc()
  
  ## Process 
  final_data_cube = validated_int_data_cube %>% 
    process_final_admin_data_cube(., context)
  gc()
    
}

```

Graet now this data cube is validated. Let's move on to the metadata.


# 2. Metadata Standardization


## 2.1 Setup Metadata Template

The complexity of SALURBAL is that there are different types of rleationships between data points and metadata fields. For the SALURBAL renovaiton the first step of metadata is to clearly for each field define what composite keys are used to link the metadata to the data points.

Please open linkage.csv and enter this information. In our case this is a really simple dataset

- Most of the metadata exists at a dataset level
- only a few fields are existing at a variable level:
    - var_label
    - var_def
    - value_type
    - units
    - coding
- Luckily metadata does not differ by any other composite keys either such as country, strata or year

Once the linkage.csv is clearly defined. We can use the linkage.csv to generate a metadata template that is compatible with the data cube and is as noramlized as possible for ease of metadata entry. 

```{r}
## Process
template_metadata_cube = process_linkage_table(context)

## Preview
salurbal_renovation_reactable(template_metadata_cube)
```

Great now our tempalte is set up. We can see a few new files in our folder: 

- template__codebook files are the tempalte for metadata entry. This is jsut a backup of the original tmeplate for reference.
- raw__codebook. this is the template htat renovators first fill out before it is processed and reviewed. 

Let's commit here to git so we can track changes to the metadata sheets as renovators fill things in. This is often the hand off between data standardization and metadata standardization. 

## 2.2. Raw Metadata

Note here are some resources:

- [Codebook for these columns](https://www.notion.so/drexel-ccuh/228da34c473f44ac881b1e2f0efbad2d?v=aa6e48ec6e5149c2b015b433ab429d3a)
- [Domains](https://www.notion.so/drexel-ccuh/14557008e885801cb6a2c9e332ffe6bd?v=6e7fbf678153460a87e35238d0c2a6c8)
- [Subdomains](https://www.notion.so/drexel-ccuh/14557008e885805ea5e9ef40bfddda2a?v=14557008e885814c87e9000c9cde7650)

### Metadata Entry

Renovators mined metadata from excel sheets and word docs to fill out codebook templates to produce raw_codebooks. After they filled it out we can import it.

```{r}
if (file.exists('4.1-raw__codebook.xlsx')){ 
  raw_codebook_object = import_salurbal_xslx_cdbk(context$raw_cdbk_path, context)
  raw_codebook_object 
}
```

### Validation to Intermediate standards

We now have a function that validate of metadata in raw codebook (e.g. correct structure, no missing values for certain columns). If the metadata is valid then it will generate the next step of int_codebooks that are sent to DMC for review.

```{r}
 
## Validation + Processing
valid_raw__cdbks = raw_codebook_object %>% 
  process_raw_metadata_object(., context)


## Snapshot
snapshot_excel(path = '4.1-raw__codebook.xlsx')
```

Looks good we have also captured these changes in json diffs. A good time to commit!

## 2.3 Reviewed Metadata

So by this time DMC has review the int_codebook and made any changes. Let's now import it and doing final processing:

- Validate: make sure its structurally correct and it passes additional metadata tests (e.g. no missing values and etc.)
- Denormalize: turn the noramlized sheets into a metadata cube that can be easily joined with the data cube

```{r}
#| code-summary: 'Review'
#| code-fold: true

## Partition paths
intermediate_codebook_object = import_salurbal_xslx_cdbk(
  context$int_cdbk_path, 
  context)

## Process codebook object into metadata cube
final_metadata_cube = intermediate_codebook_object %>% 
  process_final_metadata_cube(.,context)

## Preview
final_metadata_cube %>% 
  salurbal_renovation_reactable()
 
```

Okay. Things look good. Review by renovators, DMC and structurally valid. Time to wrap things up by loading it into our datawarehouse.


# 3. Data warehouse Loading

We want to test the data and metadAta together before it gets to DBT. Due to size limitations of many datasets we will do all integrated testing out-of-memory via Arrow. We do a few simple tests: consistency of the join and valid `public` value. After validation, the data and metadata are then loaded as DBT sources.

Lets do the integration first. Note results are cached so just delete or run manually the chunk below to refresh.

```{r}
process_preliminary_admin_cube(context)
```

Now lets do the validatiaon 

```{r}
#| code-summary: 'Valid Admin Cube'

## Validate Prelim OBT
valid_prelim_admin_cube = context %>% 
  validate_preliminary_admin_cube()
 
```

Cube is validate. Let's load into data warehouse with other cubes. This writes three types of files to the warehouse source folder:

- `{id}_data.parquet`: the data cube
- `{id}_metadata.parquet`: the metadata cube
- `{id}_loading.parquet`: the renovation metadata

```{r}
## Loading to datawarehouse
load_into_dbt_source(
  context, 
  valid_prelim_admin_cube)
```

Examine the results in the data warehouse. Here we can use a database or orchestrator like DBT to compile data and manage transformations.


 